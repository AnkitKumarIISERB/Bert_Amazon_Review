# -*- coding: utf-8 -*-
"""updatedar.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UWs_Ma1BIXBtlYoKXSX27q5YfngUuYA9
"""

!pip install -q transformers datasets torch scikit-learn pandas tqdm matplotlib seaborn

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments
import torch
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_recall_fscore_support
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv("Reviews.csv")
df = df[['Text', 'Score']].dropna()
print("Dataset size:", len(df))
print(df.head())

def label_sentiment(score):
    if score <= 2:
        return 0  # Negative
    elif score == 3:
        return 1  # Neutral
    else:
        return 2  # Positive

df['label'] = df['Score'].apply(label_sentiment)
df = df[['Text', 'label']]

plt.figure(figsize=(6,4))
sns.countplot(x=df['label'], palette='Set2')
plt.title("Original Label Distribution (Imbalanced)")
plt.xlabel("Sentiment Label (0=Neg, 1=Neutral, 2=Pos)")
plt.ylabel("Count")
plt.show()

print("\nLabel distribution before balancing:\n", df['label'].value_counts())

min_size = df['label'].value_counts().min()
balanced_df = (
    df.groupby('label', group_keys=False)
      .apply(lambda x: x.sample(min_size, random_state=42))
      .reset_index(drop=True)
)

plt.figure(figsize=(6,4))
sns.countplot(x=balanced_df['label'], palette='Set1')
plt.title("Balanced Label Distribution (Sampled for Fast Training)")
plt.xlabel("Sentiment Label")
plt.ylabel("Count")
plt.show()

print("\nBalanced label distribution:\n", balanced_df['label'].value_counts())

train_df, test_df = train_test_split(balanced_df, test_size=0.1, random_state=42)

tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

def tokenize(batch):
    return tokenizer(batch['Text'], padding='max_length', truncation=True, max_length=64)

train_dataset = Dataset.from_pandas(train_df).map(tokenize, batched=True)
test_dataset = Dataset.from_pandas(test_df).map(tokenize, batched=True)

train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])
test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])

model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir='./results',
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=1,
    logging_dir='./logs',
    logging_steps=100,
    save_total_limit=2,

)

def compute_metrics(pred):
    labels = pred.label_ids
    preds = np.argmax(pred.predictions, axis=1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "f1": f1, "precision": precision, "recall": recall}

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics
)

trainer.train()

predictions = trainer.predict(test_dataset)
y_pred = np.argmax(predictions.predictions, axis=1)
y_true = predictions.label_ids

acc = accuracy_score(y_true, y_pred)
report = classification_report(y_true, y_pred, target_names=['Negative', 'Neutral', 'Positive'])
print(f"\n✅ Accuracy: {acc*100:.2f}%\n")
print("Classification Report:\n", report)

cm = confusion_matrix(y_true, y_pred)
labels = ['Negative', 'Neutral', 'Positive']

plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Purples', xticklabels=labels, yticklabels=labels)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix - Amazon Reviews Sentiment')
plt.show()

metrics_dict = compute_metrics(predictions)
plt.figure(figsize=(6,4))
sns.barplot(x=list(metrics_dict.keys()), y=list(metrics_dict.values()), palette="cool")
plt.title("Model Performance Metrics")
plt.ylim(0,1)
plt.show()

def predict_sentiment(text):
    model.eval()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=128)
    inputs = {key: val.to(device) for key, val in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)
        pred = torch.argmax(outputs.logits, dim=1).item()

    labels = ['Negative', 'Neutral', 'Positive']
    return labels[pred]
print("\n--- Example Predictions ---")
print("Text: This product is amazing! →", predict_sentiment("This product is amazing!"))
print("Text: Terrible quality, broke after one use. →", predict_sentiment("Terrible quality, broke after one use."))
print("Text: It’s okay, not great but not bad either. →", predict_sentiment("It’s okay, not great but not bad either."))

model.save_pretrained("./distilbert_amazon_model")
tokenizer.save_pretrained("./distilbert_amazon_tokenizer")

print("\n✅ Model and tokenizer saved successfully!")

